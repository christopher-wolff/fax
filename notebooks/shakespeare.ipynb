{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "print('Ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/tiny_shakespeare'\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'tiny_shakespeare.txt')\n",
    "TRAIN_SPLIT_SIZE = 0.9\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "SEQLEN = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_PATH):\n",
    "    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(DATASET_PATH, 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "print(f\"Vocab: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings between characters and integers.\n",
    "stoi = { char: i for i, char in enumerate(chars) }\n",
    "itos = { i: char for i, char in enumerate(chars) }\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test splits.\n",
    "n = len(raw_data)\n",
    "train_data = raw_data[:int(n * TRAIN_SPLIT_SIZE)]\n",
    "val_data = raw_data[int(n * TRAIN_SPLIT_SIZE):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split has 1,003,854 tokens\n",
      "Val split has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# Encode both to integers.\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"Train split has {len(train_ids):,} tokens\")\n",
    "print(f\"Val split has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to bin files.\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(DATA_DIR, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(DATA_DIR, 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the meta information as well, to help us encode and decode later.\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(DATA_DIR, 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class Batch:\n",
    "    inputs: chex.ArrayDevice\n",
    "    targets: chex.ArrayDevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(DATA_DIR, 'train.bin')\n",
    "val_path = os.path.join(DATA_DIR, 'val.bin')\n",
    "\n",
    "train_data = np.memmap(train_path, dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(val_path, dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split) -> Batch:\n",
    "    assert split in ['train', 'val']\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idx = np.random.randint(\n",
    "        low=0,\n",
    "        high=len(data) - SEQLEN,\n",
    "        size=(BATCH_SIZE,),\n",
    "    )\n",
    "    inputs = np.stack([data[i:i + SEQLEN] for i in idx]).astype(np.int64)\n",
    "    targets = np.stack([\n",
    "        data[i + 1:i + 1 + SEQLEN] for i in idx\n",
    "    ]).astype(np.int64)\n",
    "    return Batch(inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: (5, 7) int64\n",
      "Targets: (5, 7) int64\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch('train')\n",
    "print('Inputs:', batch.inputs.shape, batch.inputs.dtype)\n",
    "print('Targets:', batch.targets.shape, batch.inputs.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a small Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "import fax\n",
    "from fax import modules\n",
    "\n",
    "for module in [\n",
    "    fax.initializers,\n",
    "    fax.modules.utils,\n",
    "    fax.modules.transformer.transformer,\n",
    "    fax.modules.transformer.config,\n",
    "    fax.modules,\n",
    "]:\n",
    "    importlib.reload(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(seed=0)\n",
    "cfg = modules.transformer.config.TransformerConfig(\n",
    "    d_model=8,\n",
    "    d_head=12,\n",
    "    d_ff=16,\n",
    "    n_heads=4,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layers=1,\n",
    "    max_len=SEQLEN,\n",
    ")\n",
    "transformer = modules.Transformer(key=key, cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ms ± 43.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit transformer(batch.inputs).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 µs ± 82.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "transformer_jit = jax.jit(transformer)\n",
    "transformer_jit(batch.inputs)  # compile\n",
    "%timeit transformer_jit(batch.inputs).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DEVICES = 2\n",
    "\n",
    "multi_inputs = np.broadcast_to(batch.inputs[None, ...], (N_DEVICES, BATCH_SIZE, SEQLEN))\n",
    "multi_targets = np.broadcast_to(batch.targets[None, ...], (N_DEVICES, BATCH_SIZE, SEQLEN))\n",
    "multi_batch = Batch(inputs=multi_inputs, targets=multi_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 7, 65)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_vmap = jax.vmap(transformer)\n",
    "transformer_vmap(multi_batch.inputs).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fax-env",
   "language": "python",
   "name": "fax-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b17d8c3df2e35366b2a8f0de3572370e2e9be7084845d1b488447a3edc2d4a37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
